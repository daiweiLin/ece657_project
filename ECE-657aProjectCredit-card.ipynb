{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithms on Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9   ...         V20       V21       V22       V23  \\\n",
       "0  0.098698  0.363787   ...    0.251412 -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425   ...   -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654   ...    0.524980  0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024   ...   -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739   ...    0.408542 -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd  \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import collections\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "df= pd.read_csv(\"/Users/navnigupta/Downloads/creditcard.csv\")\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "pd.DataFrame(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Under Sampling\n",
    "\n",
    "In this we have done under sampling to deal with the highly imbalanced dataset. So under sampling gives equal number of majority and minority class samples which are equal to number of minority class samples of the original dataset.<br>\n",
    "Here we have two classes fraudelent and non-fraudalent where fraudelent is the minority class represented by 1 and non-fraudalent is majority class represented by 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 492), (1, 492)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>162183.0</td>\n",
       "      <td>2.049094</td>\n",
       "      <td>0.186189</td>\n",
       "      <td>-1.707198</td>\n",
       "      <td>0.530768</td>\n",
       "      <td>0.160589</td>\n",
       "      <td>-1.448570</td>\n",
       "      <td>0.239310</td>\n",
       "      <td>-0.353611</td>\n",
       "      <td>0.634425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197782</td>\n",
       "      <td>0.741141</td>\n",
       "      <td>-0.009744</td>\n",
       "      <td>-0.085057</td>\n",
       "      <td>0.228384</td>\n",
       "      <td>-0.097292</td>\n",
       "      <td>-0.001028</td>\n",
       "      <td>-0.032390</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120967.0</td>\n",
       "      <td>2.125540</td>\n",
       "      <td>-0.030714</td>\n",
       "      <td>-1.527653</td>\n",
       "      <td>0.121046</td>\n",
       "      <td>0.543172</td>\n",
       "      <td>-0.347988</td>\n",
       "      <td>0.157221</td>\n",
       "      <td>-0.229126</td>\n",
       "      <td>0.477999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.336497</td>\n",
       "      <td>-0.838932</td>\n",
       "      <td>0.275173</td>\n",
       "      <td>0.049145</td>\n",
       "      <td>-0.156765</td>\n",
       "      <td>0.205919</td>\n",
       "      <td>-0.072321</td>\n",
       "      <td>-0.059009</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26484.0</td>\n",
       "      <td>-4.155859</td>\n",
       "      <td>-5.705748</td>\n",
       "      <td>0.274699</td>\n",
       "      <td>-0.993262</td>\n",
       "      <td>-6.059393</td>\n",
       "      <td>5.210848</td>\n",
       "      <td>5.811316</td>\n",
       "      <td>0.367888</td>\n",
       "      <td>1.750710</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371671</td>\n",
       "      <td>1.195815</td>\n",
       "      <td>4.188762</td>\n",
       "      <td>-1.091077</td>\n",
       "      <td>1.033044</td>\n",
       "      <td>0.224493</td>\n",
       "      <td>-0.486741</td>\n",
       "      <td>0.194275</td>\n",
       "      <td>1937.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65751.0</td>\n",
       "      <td>-0.566420</td>\n",
       "      <td>-0.579576</td>\n",
       "      <td>0.823503</td>\n",
       "      <td>-1.451240</td>\n",
       "      <td>-0.583587</td>\n",
       "      <td>0.206381</td>\n",
       "      <td>1.601392</td>\n",
       "      <td>-0.370446</td>\n",
       "      <td>-1.910354</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065082</td>\n",
       "      <td>-0.761357</td>\n",
       "      <td>0.641524</td>\n",
       "      <td>-0.568974</td>\n",
       "      <td>-0.053164</td>\n",
       "      <td>-0.690995</td>\n",
       "      <td>-0.228630</td>\n",
       "      <td>-0.157254</td>\n",
       "      <td>320.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137025.0</td>\n",
       "      <td>0.060858</td>\n",
       "      <td>-0.261762</td>\n",
       "      <td>-1.699493</td>\n",
       "      <td>-1.202327</td>\n",
       "      <td>3.699527</td>\n",
       "      <td>3.196249</td>\n",
       "      <td>0.437208</td>\n",
       "      <td>0.421541</td>\n",
       "      <td>0.492435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>0.534602</td>\n",
       "      <td>0.089602</td>\n",
       "      <td>0.667918</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.611584</td>\n",
       "      <td>-0.469946</td>\n",
       "      <td>-0.514370</td>\n",
       "      <td>11.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  162183.0  2.049094  0.186189 -1.707198  0.530768  0.160589 -1.448570   \n",
       "1  120967.0  2.125540 -0.030714 -1.527653  0.121046  0.543172 -0.347988   \n",
       "2   26484.0 -4.155859 -5.705748  0.274699 -0.993262 -6.059393  5.210848   \n",
       "3   65751.0 -0.566420 -0.579576  0.823503 -1.451240 -0.583587  0.206381   \n",
       "4  137025.0  0.060858 -0.261762 -1.699493 -1.202327  3.699527  3.196249   \n",
       "\n",
       "         V7        V8        V9  ...         V21       V22       V23  \\\n",
       "0  0.239310 -0.353611  0.634425  ...    0.197782  0.741141 -0.009744   \n",
       "1  0.157221 -0.229126  0.477999  ...   -0.336497 -0.838932  0.275173   \n",
       "2  5.811316  0.367888  1.750710  ...    1.371671  1.195815  4.188762   \n",
       "3  1.601392 -0.370446 -1.910354  ...   -0.065082 -0.761357  0.641524   \n",
       "4  0.437208  0.421541  0.492435  ...    0.008303  0.534602  0.089602   \n",
       "\n",
       "        V24       V25       V26       V27       V28   Amount  Class  \n",
       "0 -0.085057  0.228384 -0.097292 -0.001028 -0.032390     2.99      0  \n",
       "1  0.049145 -0.156765  0.205919 -0.072321 -0.059009     1.98      0  \n",
       "2 -1.091077  1.033044  0.224493 -0.486741  0.194275  1937.66      0  \n",
       "3 -0.568974 -0.053164 -0.690995 -0.228630 -0.157254   320.05      0  \n",
       "4  0.667918  0.017798  0.611584 -0.469946 -0.514370    11.50      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled_U, y_resampled_U = rus.fit_resample(X, y)\n",
    "X_resampled_df1=pd.DataFrame(X_resampled_U)\n",
    "y_resampled_df1=pd.DataFrame(y_resampled_U)\n",
    "y_resampled_df1=y_resampled_df1.rename(columns={0:30})\n",
    "df_new1=pd.concat([X_resampled_df1, y_resampled_df1], axis=1)\n",
    "print(sorted(collections.Counter(y_resampled_U).items()))\n",
    "df_new1.columns=df.keys()\n",
    "df_new1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>162183.0</td>\n",
       "      <td>2.049094</td>\n",
       "      <td>0.186189</td>\n",
       "      <td>-1.707198</td>\n",
       "      <td>0.530768</td>\n",
       "      <td>0.160589</td>\n",
       "      <td>-1.448570</td>\n",
       "      <td>0.239310</td>\n",
       "      <td>-0.353611</td>\n",
       "      <td>0.634425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232666</td>\n",
       "      <td>0.197782</td>\n",
       "      <td>0.741141</td>\n",
       "      <td>-0.009744</td>\n",
       "      <td>-0.085057</td>\n",
       "      <td>0.228384</td>\n",
       "      <td>-0.097292</td>\n",
       "      <td>-0.001028</td>\n",
       "      <td>-0.032390</td>\n",
       "      <td>2.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120967.0</td>\n",
       "      <td>2.125540</td>\n",
       "      <td>-0.030714</td>\n",
       "      <td>-1.527653</td>\n",
       "      <td>0.121046</td>\n",
       "      <td>0.543172</td>\n",
       "      <td>-0.347988</td>\n",
       "      <td>0.157221</td>\n",
       "      <td>-0.229126</td>\n",
       "      <td>0.477999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.125804</td>\n",
       "      <td>-0.336497</td>\n",
       "      <td>-0.838932</td>\n",
       "      <td>0.275173</td>\n",
       "      <td>0.049145</td>\n",
       "      <td>-0.156765</td>\n",
       "      <td>0.205919</td>\n",
       "      <td>-0.072321</td>\n",
       "      <td>-0.059009</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26484.0</td>\n",
       "      <td>-4.155859</td>\n",
       "      <td>-5.705748</td>\n",
       "      <td>0.274699</td>\n",
       "      <td>-0.993262</td>\n",
       "      <td>-6.059393</td>\n",
       "      <td>5.210848</td>\n",
       "      <td>5.811316</td>\n",
       "      <td>0.367888</td>\n",
       "      <td>1.750710</td>\n",
       "      <td>...</td>\n",
       "      <td>3.944592</td>\n",
       "      <td>1.371671</td>\n",
       "      <td>1.195815</td>\n",
       "      <td>4.188762</td>\n",
       "      <td>-1.091077</td>\n",
       "      <td>1.033044</td>\n",
       "      <td>0.224493</td>\n",
       "      <td>-0.486741</td>\n",
       "      <td>0.194275</td>\n",
       "      <td>1937.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65751.0</td>\n",
       "      <td>-0.566420</td>\n",
       "      <td>-0.579576</td>\n",
       "      <td>0.823503</td>\n",
       "      <td>-1.451240</td>\n",
       "      <td>-0.583587</td>\n",
       "      <td>0.206381</td>\n",
       "      <td>1.601392</td>\n",
       "      <td>-0.370446</td>\n",
       "      <td>-1.910354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977739</td>\n",
       "      <td>-0.065082</td>\n",
       "      <td>-0.761357</td>\n",
       "      <td>0.641524</td>\n",
       "      <td>-0.568974</td>\n",
       "      <td>-0.053164</td>\n",
       "      <td>-0.690995</td>\n",
       "      <td>-0.228630</td>\n",
       "      <td>-0.157254</td>\n",
       "      <td>320.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137025.0</td>\n",
       "      <td>0.060858</td>\n",
       "      <td>-0.261762</td>\n",
       "      <td>-1.699493</td>\n",
       "      <td>-1.202327</td>\n",
       "      <td>3.699527</td>\n",
       "      <td>3.196249</td>\n",
       "      <td>0.437208</td>\n",
       "      <td>0.421541</td>\n",
       "      <td>0.492435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014550</td>\n",
       "      <td>0.008303</td>\n",
       "      <td>0.534602</td>\n",
       "      <td>0.089602</td>\n",
       "      <td>0.667918</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.611584</td>\n",
       "      <td>-0.469946</td>\n",
       "      <td>-0.514370</td>\n",
       "      <td>11.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  162183.0  2.049094  0.186189 -1.707198  0.530768  0.160589 -1.448570   \n",
       "1  120967.0  2.125540 -0.030714 -1.527653  0.121046  0.543172 -0.347988   \n",
       "2   26484.0 -4.155859 -5.705748  0.274699 -0.993262 -6.059393  5.210848   \n",
       "3   65751.0 -0.566420 -0.579576  0.823503 -1.451240 -0.583587  0.206381   \n",
       "4  137025.0  0.060858 -0.261762 -1.699493 -1.202327  3.699527  3.196249   \n",
       "\n",
       "         V7        V8        V9   ...          V20       V21       V22  \\\n",
       "0  0.239310 -0.353611  0.634425   ...    -0.232666  0.197782  0.741141   \n",
       "1  0.157221 -0.229126  0.477999   ...    -0.125804 -0.336497 -0.838932   \n",
       "2  5.811316  0.367888  1.750710   ...     3.944592  1.371671  1.195815   \n",
       "3  1.601392 -0.370446 -1.910354   ...     0.977739 -0.065082 -0.761357   \n",
       "4  0.437208  0.421541  0.492435   ...    -0.014550  0.008303  0.534602   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28   Amount  \n",
       "0 -0.009744 -0.085057  0.228384 -0.097292 -0.001028 -0.032390     2.99  \n",
       "1  0.275173  0.049145 -0.156765  0.205919 -0.072321 -0.059009     1.98  \n",
       "2  4.188762 -1.091077  1.033044  0.224493 -0.486741  0.194275  1937.66  \n",
       "3  0.641524 -0.568974 -0.053164 -0.690995 -0.228630 -0.157254   320.05  \n",
       "4  0.089602  0.667918  0.017798  0.611584 -0.469946 -0.514370    11.50  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new=pd.DataFrame(X_resampled_U,columns=X.keys())\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data into test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_resampled_U, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:  3 train:  0.9484066767830045\n",
      "depth:  3 test:  0.9261538461538461\n"
     ]
    }
   ],
   "source": [
    "max_test_score = 0\n",
    "MaxDepth=[]\n",
    "TestScore=[]\n",
    "TrainScore=[]\n",
    "for i in range(1, 20):\n",
    "    MaxDepth.append(i)\n",
    "    clf_dt = DecisionTreeClassifier(max_depth=i)\n",
    "    clf_dt.fit(X_train, y_train)\n",
    "    train_score = clf_dt.score(X_train, y_train)\n",
    "    test_score = clf_dt.score(X_test, y_test)\n",
    "    TestScore.append(test_score)\n",
    "    TrainScore.append(train_score)\n",
    "    if test_score > max_test_score:\n",
    "        related_train_score = train_score\n",
    "        max_test_score = test_score\n",
    "        max_i = i\n",
    "        best_clf_dt = clf_dt\n",
    "print(\"depth: \", max_i, \"train: \", related_train_score)\n",
    "print(\"depth: \", max_i, \"test: \", max_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score is 0.9590288315629742\n"
     ]
    }
   ],
   "source": [
    "clf_gini = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5, max_features=None, max_leaf_nodes=None, min_samples_leaf=5, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best')\n",
    "clf_gini.fit(X_train, y_train)\n",
    "print(\"The train score is\",clf_gini.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_en_gini = clf_gini.predict(X_test)\n",
    "y_pred_en_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[152  12]\n",
      " [ 16 145]]\n",
      "\t\tClassification Report for CART\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92       164\n",
      "           1       0.92      0.90      0.91       161\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       325\n",
      "   macro avg       0.91      0.91      0.91       325\n",
      "weighted avg       0.91      0.91      0.91       325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_en_gini))  \n",
    "print(\"\\t\\tClassification Report for CART\")\n",
    "print(classification_report(y_test, y_pred_en_gini)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.5 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data With Rules 0.9494949494949495\n",
      "f1_score of Test Data With Rules 0.9523809523809523\n",
      "recall_score of Test Data With Rules 0.9259259259259259\n",
      "before pruning: 539\n",
      "before pruning: 300\n",
      "Accuracy of Test Data With Rules 0.9090909090909091\n",
      "f1_score of Test Data With Rules 0.9142857142857144\n",
      "recall_score of Test Data With Rules 0.8727272727272727\n",
      "before pruning: 758\n",
      "before pruning: 423\n",
      "Accuracy of Test Data With Rules 0.8585858585858586\n",
      "f1_score of Test Data With Rules 0.8727272727272727\n",
      "recall_score of Test Data With Rules 0.8571428571428571\n",
      "before pruning: 689\n",
      "before pruning: 299\n",
      "Accuracy of Test Data With Rules 0.8585858585858586\n",
      "f1_score of Test Data With Rules 0.8541666666666666\n",
      "recall_score of Test Data With Rules 0.9111111111111111\n",
      "before pruning: 636\n",
      "before pruning: 454\n",
      "Accuracy of Test Data With Rules 0.9183673469387755\n",
      "f1_score of Test Data With Rules 0.9166666666666666\n",
      "recall_score of Test Data With Rules 0.9361702127659575\n",
      "before pruning: 885\n",
      "before pruning: 491\n",
      "Accuracy of Test Data With Rules 0.8877551020408163\n",
      "f1_score of Test Data With Rules 0.8817204301075268\n",
      "recall_score of Test Data With Rules 0.9318181818181818\n",
      "before pruning: 789\n",
      "before pruning: 452\n",
      "Accuracy of Test Data With Rules 0.8775510204081632\n",
      "f1_score of Test Data With Rules 0.8749999999999999\n",
      "recall_score of Test Data With Rules 0.9130434782608695\n",
      "before pruning: 865\n",
      "before pruning: 665\n",
      "Accuracy of Test Data With Rules 0.8775510204081632\n",
      "f1_score of Test Data With Rules 0.8378378378378379\n",
      "recall_score of Test Data With Rules 0.775\n",
      "before pruning: 709\n",
      "before pruning: 387\n",
      "Accuracy of Test Data With Rules 0.8979591836734694\n",
      "f1_score of Test Data With Rules 0.9019607843137256\n",
      "recall_score of Test Data With Rules 0.9387755102040817\n",
      "before pruning: 934\n",
      "before pruning: 675\n",
      "Accuracy of Test Data With Rules 0.9489795918367347\n",
      "f1_score of Test Data With Rules 0.9541284403669724\n",
      "recall_score of Test Data With Rules 0.9285714285714286\n",
      "before pruning: 835\n",
      "before pruning: 560\n",
      "Accuracy of Test Data With Rules 0.9090909090909091\n",
      "f1_score of Test Data With Rules 0.9142857142857143\n",
      "recall_score of Test Data With Rules 0.9056603773584906\n",
      "before pruning: 1005\n",
      "before pruning: 758\n",
      "Accuracy of Test Data With Rules 0.9191919191919192\n",
      "f1_score of Test Data With Rules 0.92\n",
      "recall_score of Test Data With Rules 0.9387755102040817\n",
      "before pruning: 749\n",
      "before pruning: 388\n",
      "Accuracy of Test Data With Rules 0.8787878787878788\n",
      "f1_score of Test Data With Rules 0.8636363636363636\n",
      "recall_score of Test Data With Rules 0.8837209302325582\n",
      "before pruning: 778\n",
      "before pruning: 518\n",
      "Accuracy of Test Data With Rules 0.8181818181818182\n",
      "f1_score of Test Data With Rules 0.8301886792452831\n",
      "recall_score of Test Data With Rules 0.8627450980392157\n",
      "before pruning: 670\n",
      "before pruning: 427\n",
      "Accuracy of Test Data With Rules 0.8775510204081632\n",
      "f1_score of Test Data With Rules 0.8775510204081632\n",
      "recall_score of Test Data With Rules 0.8269230769230769\n",
      "before pruning: 710\n",
      "before pruning: 390\n",
      "Accuracy of Test Data With Rules 0.9183673469387755\n",
      "f1_score of Test Data With Rules 0.9111111111111111\n",
      "recall_score of Test Data With Rules 0.9111111111111111\n",
      "before pruning: 868\n",
      "before pruning: 472\n",
      "Accuracy of Test Data With Rules 0.9081632653061225\n",
      "f1_score of Test Data With Rules 0.9230769230769231\n",
      "recall_score of Test Data With Rules 0.9473684210526315\n",
      "before pruning: 863\n",
      "before pruning: 598\n",
      "Accuracy of Test Data With Rules 0.8877551020408163\n",
      "f1_score of Test Data With Rules 0.8865979381443299\n",
      "recall_score of Test Data With Rules 0.8775510204081632\n",
      "before pruning: 781\n",
      "before pruning: 567\n",
      "Accuracy of Test Data With Rules 0.8571428571428571\n",
      "f1_score of Test Data With Rules 0.8541666666666666\n",
      "recall_score of Test Data With Rules 0.9111111111111111\n",
      "before pruning: 709\n",
      "before pruning: 356\n",
      "Accuracy of Test Data With Rules 0.8877551020408163\n",
      "f1_score of Test Data With Rules 0.8910891089108911\n",
      "recall_score of Test Data With Rules 0.9375\n",
      "before pruning: 578\n",
      "before pruning: 286\n",
      "Mean Accuracy So Far on Test: 0.6604308390022676\n",
      "The Mean Accuracy of Classifier 0.8922954030096888\n",
      "The Mean Variance of Classifier 0.0009656762185143674\n",
      "Mean f1_score So Far on Test: 0.8916289145419392\n",
      "Mean recall_score So Far on Test: 0.8996376317484064\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "parent_node = None\n",
    "# the node class that will make up the tree\n",
    "class decisionTreeNode():\n",
    "    def __init__(self, is_leaf_node, classification, attribute_split_value, parent, left_child, right_child, height):\n",
    "\n",
    "        self.classification = None\n",
    "        self.attribute_split = None\n",
    "        self.attribute_split_value = None\n",
    "        self.parent = parent\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.height = None\n",
    "        self.is_leaf_node = True\n",
    "\n",
    "\n",
    "\n",
    "#Split the data based on the feature and a value to data above and data below\n",
    "def split_data(data, split_column, split_value):\n",
    "    \n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    data_below = data[split_column_values <= split_value]\n",
    "    data_above = data[split_column_values >  split_value]\n",
    "    \n",
    "    return data_below, data_above\n",
    "\n",
    "#Get all the boundary values for each features (Key is feature and values are the splits)\n",
    "def get_potential_splits(data):\n",
    "    \n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    for column_index in range(n_columns - 1):        # excluding the last column which is the label\n",
    "        potential_splits[column_index] = []\n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "\n",
    "        for index in range(len(unique_values)):\n",
    "            if index != 0:\n",
    "                current_value = unique_values[index]\n",
    "                previous_value = unique_values[index - 1]\n",
    "                potential_split = (current_value + previous_value) / 2\n",
    "                \n",
    "                potential_splits[column_index].append(potential_split)\n",
    "        if (unique_values.shape[0] == 1):\n",
    "            potential_split = unique_values[index]\n",
    "            \n",
    "            potential_splits[column_index].append(potential_split)\n",
    "\n",
    "    \n",
    "    return potential_splits\n",
    "\n",
    "#Calculates Entropy of the data given\n",
    "def calculate_entropy(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "     \n",
    "    return entropy\n",
    "\n",
    "#Calculates the entropy of data below and data above\n",
    "def calculate_overall_entropy(data_below, data_above):\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_entropy =  (p_data_below * calculate_entropy(data_below) \n",
    "                      + p_data_above * calculate_entropy(data_above))\n",
    "    \n",
    "    return overall_entropy\n",
    "\n",
    "#Check if all data is of same class\n",
    "def check_purity(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes = np.unique(label_column)\n",
    "\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#Classify data based on majority\n",
    "def classify_data(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    index = counts_unique_classes.argmax()\n",
    "    classification = unique_classes[index]\n",
    "    \n",
    "    return classification\n",
    "\n",
    "#Gives the best feature and its split value after checking all features based on gain ratio\n",
    "def determine_best_split(data, potential_splits):\n",
    "    \n",
    "    entropy_label = calculate_entropy(data)   \n",
    "    overall_gain = -1.0\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n",
    "            current_information_gain = entropy_label - current_overall_entropy\n",
    "            current_splitting_info = splitting_information(data_below,data_above)\n",
    "            if current_splitting_info == 0:\n",
    "                current_gain_ratio = 0\n",
    "            else:\n",
    "                current_gain_ratio = float(current_information_gain / current_splitting_info)\n",
    "\n",
    "            if current_gain_ratio >= overall_gain:\n",
    "                overall_gain = current_gain_ratio\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value\n",
    "\n",
    "#Calculates the splitting Info of data above and below for that boundary value\n",
    "def splitting_information(data_below,data_above):\n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below)/ n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    if p_data_below == 0:\n",
    "        splitting_info = p_data_above * np.log2(p_data_above)\n",
    "    elif p_data_above == 0:\n",
    "        splitting_info = p_data_below * np.log2(p_data_below)\n",
    "    else:\n",
    "        splitting_info = -p_data_below * np.log2(p_data_below) -p_data_above * np.log2(p_data_above) \n",
    "    \n",
    "    return splitting_info\n",
    "\n",
    "def decision_tree_algorithm(df, parent_node,counter=0, min_samples=3):\n",
    "    node = decisionTreeNode(True, None, None, parent_node, None, None, 0)\n",
    "    # data preparations\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        data = df.values\n",
    "    else:\n",
    "        data = df           \n",
    "    \n",
    "    \n",
    "    # base cases\n",
    "    if (check_purity(data)) or (len(data) < min_samples):\n",
    "        classification = classify_data(data)\n",
    "        node.is_leaf_node = True\n",
    "        node.classification = classification\n",
    "        return node\n",
    "\n",
    "    \n",
    "    # recursive part\n",
    "    else:    \n",
    "        counter += 1\n",
    "\n",
    "        # helper functions \n",
    "        potential_splits = get_potential_splits(data)\n",
    "        split_column, split_value = determine_best_split(data, potential_splits)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        node.is_leaf_node = False\n",
    "        # instantiate sub-tree\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        \n",
    "        question = \"{} <= {}\".format(feature_name, split_value)\n",
    "\n",
    "\n",
    "       \n",
    "        if (parent_node == None):\n",
    "            node.height = 0\n",
    "        else:\n",
    "            node.parent = parent_node\n",
    "            node.height = node.parent.height + 1\n",
    "\n",
    "\n",
    "        node.attribute_split = feature_name\n",
    "        node.attribute_split_value = split_value\n",
    "\n",
    "        # find answers (recursion)\n",
    "        node.left_child = decision_tree_algorithm(data_below,node, counter, min_samples)\n",
    "        node.right_child = decision_tree_algorithm(data_above,node, counter, min_samples)\n",
    "        \n",
    "\n",
    "        return node\n",
    "\n",
    "def get_paths(root, path, pathlen,all_paths,val):\n",
    "    if (root==None):\n",
    "        return\n",
    "    \n",
    "    if root.is_leaf_node == True: \n",
    "        path.append(root.classification) \n",
    "    else:\n",
    "        path.append('row[\\'' + root.attribute_split + '\\']' + val + str(root.attribute_split_value))\n",
    "        \n",
    "    pathlen= pathlen+1\n",
    "    if (root.left_child == None and root.right_child == None): # If leaf, append current path\n",
    "        add = path[:]\n",
    "        all_paths.append(add)\n",
    "        path.pop()\n",
    "        root = root.parent\n",
    "    else:\n",
    "        get_paths(root.left_child, path, pathlen,all_paths,' <= ')\n",
    "        path[pathlen-1]= 'row[\\'' + root.attribute_split + '\\']' +' > ' + str(root.attribute_split_value)\n",
    "        get_paths(root.right_child, path,pathlen,all_paths,' <= ')\n",
    "        path.pop()\n",
    "\n",
    "    return all_paths\n",
    "\n",
    "def classify_test_data(root,data):\n",
    "    predictions = []\n",
    "    tree = root \n",
    "    data = data.iloc[:, :-1]\n",
    "    for index, sample in data.iterrows():\n",
    "        root = tree\n",
    "        while(tree.is_leaf_node!=True):\n",
    "            if (sample.loc[tree.attribute_split] <= tree.attribute_split_value):\n",
    "                tree = tree.left_child\n",
    "            else:\n",
    "                tree = tree.right_child\n",
    "        predictions.append(tree.classification)\n",
    "        tree = root\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def calc_accuracy_rule(rule,test):\n",
    "    wrong = 0\n",
    "#Check how many classified correctly\n",
    "    for index, row in test.iterrows():\n",
    "        s=0\n",
    "        while(s<len(rule)-1):\n",
    "            if (eval(rule[s])== False):\n",
    "                wrong += 1\n",
    "                break\n",
    "            s=s+1\n",
    "    #Initial Accuracy of one Rule before pruning\n",
    "    accuracy = (test.shape[0]-wrong) / test.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def recursive_len(item):\n",
    "    if type(item) == list:\n",
    "        return sum(recursive_len(subitem) for subitem in item)\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def prune(all_rules,val_data):\n",
    "    acc_rlist = []\n",
    "    maping = []\n",
    "    rulenos = []\n",
    "    #What are the labels in my val data\n",
    "    ctoprune = val_data['Class'].unique()\n",
    "#     Loop at all rules one by one\n",
    "    size_of_rules = recursive_len(all_rules)\n",
    "    print(\"before pruning:\",size_of_rules)\n",
    "    for i in range(len(all_rules)):\n",
    "        init_accuracy = 0\n",
    "        #Loop only on the rules applicable to my valset\n",
    "        if all_rules[i][-1] in ctoprune:\n",
    "                #Get the label of the Rule\n",
    "                label = all_rules[i][-1]\n",
    "                #Get all samples for that label\n",
    "                test = val_data[val_data['Class']==label]\n",
    "                #Check Initial Accuracy of the rule\n",
    "                init_accuracy = calc_accuracy_rule(all_rules[i],test)\n",
    "                \n",
    "                temp = all_rules[i][:]\n",
    "                pruned_accuracy = -1\n",
    "                while (init_accuracy!=pruned_accuracy):\n",
    "                # if (init_accuracy!=pruned_accuracy):\n",
    "                    for x in range(len(all_rules[i])-1):\n",
    "                        del temp[x]\n",
    "                        accuracy = calc_accuracy_rule(temp,test)\n",
    "                        if accuracy > init_accuracy:\n",
    "                            delx = x\n",
    "                            init_accuracy = accuracy\n",
    "                        temp = all_rules[i][:]\n",
    "                    # Ensure variable is defined\n",
    "                    try:\n",
    "                        delx\n",
    "                    except NameError:\n",
    "                        delx = None\n",
    "\n",
    "                    if delx is not None:\n",
    "                        del all_rules[i][delx]\n",
    "                        del delx\n",
    "                        # pruned_accuracy = init_accuracy\n",
    "                        if (len(all_rules[i])== 2):\n",
    "                            pruned_accuracy = init_accuracy\n",
    "                    else:\n",
    "                        pruned_accuracy = init_accuracy\n",
    "        else:\n",
    "            pruned_accuracy = init_accuracy\n",
    "        acc_rlist.append(pruned_accuracy)\n",
    "        rulenos.append(i)\n",
    "    maping.append(acc_rlist)\n",
    "    maping.append(rulenos)\n",
    "    size_of_rules = recursive_len(all_rules)\n",
    "    print(\"before pruning:\",size_of_rules)\n",
    "    maping= np.array(maping)\n",
    "    maping = pd.DataFrame(maping.T)\n",
    "    maping = maping.sort_values(0,ascending=False)\n",
    "    maping = pd.DataFrame(maping)\n",
    "    return all_rules,maping\n",
    "                \n",
    "def predict_prunedtree(all_rules,test_data,maping):\n",
    "    answer = []\n",
    "    unclassified = 0\n",
    "    for index, row in test_data.iterrows():\n",
    "        for indo,valus in maping.iterrows():\n",
    "            rule = all_rules[int(valus[1])]\n",
    "            s=0\n",
    "            count = 0\n",
    "            while(s<len(rule)-1):\n",
    "                if (eval(rule[s])== True):\n",
    "                    count = count + 1\n",
    "                s = s+1\n",
    "            if (count == len(rule)-1):\n",
    "                prediction = rule[-1]\n",
    "                break\n",
    "        try:\n",
    "            prediction\n",
    "        except NameError:\n",
    "            prediction = None\n",
    "        if prediction is not None:\n",
    "            answer.append(prediction)\n",
    "            del prediction\n",
    "        else:\n",
    "            unclassified = unclassified + 1\n",
    "#     print('Unclassified Sample',unclassified)\n",
    "    return answer\n",
    "\n",
    "def predict_preprunedtree(all_rules,test_data):\n",
    "    answer = []\n",
    "    unclassified = 0\n",
    "    for index, row in test_data.iterrows():\n",
    "        # prediction = row[-1]\n",
    "        for rule in all_rules:\n",
    "            s=0\n",
    "            count = 0\n",
    "            while(s<len(rule)-1):\n",
    "                if (eval(rule[s])== True):\n",
    "                    count = count + 1\n",
    "                s = s+1\n",
    "            if (count == len(rule)-1):\n",
    "                prediction = rule[-1]\n",
    "                break\n",
    "        try:\n",
    "            prediction\n",
    "        except NameError:\n",
    "            prediction = None\n",
    "        if prediction is not None:\n",
    "            answer.append(prediction)\n",
    "            del prediction\n",
    "        else:\n",
    "            unclassified = unclassified + 1\n",
    "#     print('Unclassified Sample',unclassified)\n",
    "    return answer\n",
    "\n",
    "def add_noise2(num,data):\n",
    "\n",
    "    siz_d = data.shape[0]\n",
    "    indx = int((num * siz_d)/100)\n",
    "    for x in range(indx):\n",
    "        pick = random.randint(0,int(siz_d/2))\n",
    "        label = data.iloc[pick:,-1].values[0]\n",
    "        if label > 0:\n",
    "            data.iloc[pick:,-1] = data.iloc[pick:,-1] - 1\n",
    "        else:\n",
    "            data.iloc[pick:,-1] = data.iloc[pick:,-1] + 1\n",
    "    return data\n",
    "\n",
    "def add_noise1(num,data):\n",
    "    siz_d = data.shape[0]\n",
    "    indx = int((num * siz_d)/100)\n",
    "    count = 0\n",
    "    for x in range(indx):\n",
    "        count = count+1\n",
    "        pick = random.randint(0,siz_d)\n",
    "        label = data.iloc[pick:,-1].values[0]\n",
    "        if label > 0:\n",
    "            label = label + 1\n",
    "        else:\n",
    "            label = label - 1\n",
    "        last = data.shape[0]  \n",
    "        data = data.append(data.iloc[pick,:])\n",
    "        data.iloc[last,-1] = label\n",
    "    return data\n",
    "\n",
    "noise1_5 = []\n",
    "noise1_10 = []\n",
    "noise1_15 = []\n",
    "noise2_5 = []\n",
    "noise2_10 = []\n",
    "noise2_15 = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    meanacc= []\n",
    "    meanf1_score= []\n",
    "    meanrecall_score= []\n",
    "    maping = []\n",
    "    mean_bprun = []\n",
    "    rkf = RepeatedKFold(n_splits=10, n_repeats=2, random_state=2652124)\n",
    "    for train_index, test_index in rkf.split(df_new1):\n",
    "        parent_node = None\n",
    "        test_data = df_new1.iloc[test_index]\n",
    "        train_data,val_data= train_test_split(df_new1.iloc[train_index], test_size=0.2)\n",
    "        \n",
    "        tree = decision_tree_algorithm(train_data,parent_node)\n",
    "        all_rules = get_paths(tree,[],0,[],' <= ')\n",
    "\n",
    "\n",
    "        \n",
    "# #         #Validation\n",
    "# #         y_true= val_data.iloc[:,-1].values\n",
    "# #         y_pred = classify_test_data(tree,val_data)\n",
    "# #         accuracy = accuracy_score(y_true, y_pred)\n",
    "# #         print('Pre-Pruning Accuracy of Val data With Just Tree',accuracy)\n",
    "        \n",
    "#         # # Pre-Pruning Accuracy of Test Data With Tree\n",
    "#         # y_true= test_data.iloc[:,-1].values\n",
    "#         # y_pred = classify_test_data(tree,test_data)\n",
    "#         # accuracy = accuracy_score(y_true, y_pred)\n",
    "#         # print('Pre-Pruning Accuracy of Test data With Just Tree',accuracy)\n",
    "        accuracy = []\n",
    "        #Pre-Pruning Accuracy of Test Data With Rules\n",
    "        y_true= test_data.iloc[:,-1].values\n",
    "        answer = predict_preprunedtree(all_rules,test_data)\n",
    "        accuracy = accuracy_score(y_true, answer)\n",
    "        f1_sc = f1_score(y_true, answer)\n",
    "        recall_sc = recall_score(y_true, answer)\n",
    "        print('Accuracy of Test Data With Rules',accuracy)\n",
    "        print('f1_score of Test Data With Rules',f1_sc)\n",
    "        print('recall_score of Test Data With Rules',recall_sc)\n",
    "        #print('Number of Rules before pruning',len(all_rules))\n",
    "        mean_bprun.append(accuracy)\n",
    "        #Post Pruning Accuracy with Rules\n",
    "        all_rules,maping = prune(all_rules,val_data)\n",
    "        #print('Number of Rules after pruning',len(all_rules))\n",
    "        answer = predict_prunedtree(all_rules,test_data,maping)\n",
    "        # y_true= test_data.iloc[:,-1].values\n",
    "        accuracy = accuracy_score(y_true, answer)\n",
    "       # print('Post-Pruned Accuracy Without Noise',accuracy)\n",
    "        meanacc.append(accuracy)\n",
    "        meanf1_score.append(f1_sc)\n",
    "        meanrecall_score.append(recall_sc)\n",
    "\n",
    "        \n",
    "    print('Mean Accuracy So Far on Test:',sum(meanacc) / len(meanacc))\n",
    "    print('The Mean Accuracy of Classifier',sum(mean_bprun) / len(mean_bprun))\n",
    "    print('The Mean Variance of Classifier',np.var(np.array(mean_bprun)))\n",
    "    print('Mean f1_score So Far on Test:',sum(meanf1_score) / len(meanf1_score))\n",
    "    print('Mean recall_score So Far on Test:',sum(meanrecall_score) / len(meanrecall_score))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Oversampling\n",
    "\n",
    "In this we have done over sampling to deal with the highly imbalanced dataset. So over sampling gives equal number of majority and minority class samples which are equal to number of majority class samples of the original dataset.\n",
    "Here we have two classes fraudelent and non-fraudalent where fraudelent is the minority class represented by 1 and non-fraudalent is majority class represented by 0.\n",
    "As the size of the dataset has been increased (double the size of original dataset) and hence taking a lot of time to implement decision tree algorithms,so to deal with this we have taken a sample having equal distribution of fraudelent and non fraudelent data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 284315), (1, 284315)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled_O, y_resampled_O = SMOTE().fit_resample(X, y)\n",
    "X_resampled_df=pd.DataFrame(X_resampled_O)\n",
    "y_resampled_df=pd.DataFrame(y_resampled_O)\n",
    "#df_new=X_resampled_df.join(y_resampled_df)\n",
    "y_resampled_df=y_resampled_df.rename(columns={0:30})\n",
    "df_new=pd.concat([X_resampled_df, y_resampled_df], axis=1)\n",
    "print(sorted(collections.Counter(y_resampled_O).items()))\n",
    "df_new.columns=df.keys()\n",
    "df_new.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampled class 0 subset shape: (284, 31)\n",
      "resampled class 1 subset shape: (284, 31)\n",
      "class 1 to class 0 ratio = 1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "df_new_1 = df_new.loc[df_new['Class'] == 1]\n",
    "df_new_0 = df_new.loc[df_new['Class'] == 0]\n",
    "df_new_0_subset = df_new_0.sample(frac=0.001)\n",
    "df_new_1_subset = df_new_1.sample(frac=0.001)\n",
    "print(\"resampled class 0 subset shape: {}\".format(df_new_0_subset.shape))\n",
    "print(\"resampled class 1 subset shape: {}\".format(df_new_1_subset.shape))\n",
    "print(\"class 1 to class 0 ratio = 1 : {}\".format(df_new_0_subset.shape[0]/df_new_1_subset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91844</th>\n",
       "      <td>63651.0</td>\n",
       "      <td>1.261364</td>\n",
       "      <td>-1.570609</td>\n",
       "      <td>1.164633</td>\n",
       "      <td>-1.227305</td>\n",
       "      <td>-1.956697</td>\n",
       "      <td>0.451474</td>\n",
       "      <td>-1.687024</td>\n",
       "      <td>0.306362</td>\n",
       "      <td>-1.274451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132478</td>\n",
       "      <td>0.056615</td>\n",
       "      <td>0.037879</td>\n",
       "      <td>0.016463</td>\n",
       "      <td>0.119284</td>\n",
       "      <td>-0.188350</td>\n",
       "      <td>0.080262</td>\n",
       "      <td>0.028114</td>\n",
       "      <td>67.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149421</th>\n",
       "      <td>91207.0</td>\n",
       "      <td>-0.615437</td>\n",
       "      <td>0.540314</td>\n",
       "      <td>1.348058</td>\n",
       "      <td>0.455041</td>\n",
       "      <td>0.488695</td>\n",
       "      <td>1.043549</td>\n",
       "      <td>0.710867</td>\n",
       "      <td>0.306068</td>\n",
       "      <td>1.392721</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.535097</td>\n",
       "      <td>-1.373765</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.104334</td>\n",
       "      <td>-0.283933</td>\n",
       "      <td>-1.175535</td>\n",
       "      <td>0.109232</td>\n",
       "      <td>0.149126</td>\n",
       "      <td>99.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77816</th>\n",
       "      <td>57226.0</td>\n",
       "      <td>0.860384</td>\n",
       "      <td>-0.525465</td>\n",
       "      <td>1.252583</td>\n",
       "      <td>1.337996</td>\n",
       "      <td>-1.131000</td>\n",
       "      <td>0.207570</td>\n",
       "      <td>-0.700880</td>\n",
       "      <td>0.258361</td>\n",
       "      <td>0.768112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348380</td>\n",
       "      <td>0.753101</td>\n",
       "      <td>-0.142318</td>\n",
       "      <td>0.074414</td>\n",
       "      <td>0.209207</td>\n",
       "      <td>-0.220112</td>\n",
       "      <td>0.060673</td>\n",
       "      <td>0.061285</td>\n",
       "      <td>140.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190037</th>\n",
       "      <td>128662.0</td>\n",
       "      <td>-0.294122</td>\n",
       "      <td>1.123794</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>-0.665765</td>\n",
       "      <td>1.281603</td>\n",
       "      <td>-0.162226</td>\n",
       "      <td>1.105000</td>\n",
       "      <td>-0.192295</td>\n",
       "      <td>-0.169560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214339</td>\n",
       "      <td>-0.388622</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.044649</td>\n",
       "      <td>-0.317071</td>\n",
       "      <td>-0.578125</td>\n",
       "      <td>0.296198</td>\n",
       "      <td>0.258734</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249623</th>\n",
       "      <td>154501.0</td>\n",
       "      <td>-0.955902</td>\n",
       "      <td>1.419717</td>\n",
       "      <td>0.142302</td>\n",
       "      <td>-0.478265</td>\n",
       "      <td>0.956014</td>\n",
       "      <td>-0.234302</td>\n",
       "      <td>1.066341</td>\n",
       "      <td>-0.331764</td>\n",
       "      <td>0.454253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.462791</td>\n",
       "      <td>-0.741062</td>\n",
       "      <td>-0.232627</td>\n",
       "      <td>-1.054904</td>\n",
       "      <td>-0.122503</td>\n",
       "      <td>0.421441</td>\n",
       "      <td>0.367961</td>\n",
       "      <td>0.170138</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "91844    63651.0  1.261364 -1.570609  1.164633 -1.227305 -1.956697  0.451474   \n",
       "149421   91207.0 -0.615437  0.540314  1.348058  0.455041  0.488695  1.043549   \n",
       "77816    57226.0  0.860384 -0.525465  1.252583  1.337996 -1.131000  0.207570   \n",
       "190037  128662.0 -0.294122  1.123794  0.040958 -0.665765  1.281603 -0.162226   \n",
       "249623  154501.0 -0.955902  1.419717  0.142302 -0.478265  0.956014 -0.234302   \n",
       "\n",
       "              V7        V8        V9  ...         V21       V22       V23  \\\n",
       "91844  -1.687024  0.306362 -1.274451  ...   -0.132478  0.056615  0.037879   \n",
       "149421  0.710867  0.306068  1.392721  ...   -0.535097 -1.373765  0.233600   \n",
       "77816  -0.700880  0.258361  0.768112  ...    0.348380  0.753101 -0.142318   \n",
       "190037  1.105000 -0.192295 -0.169560  ...   -0.214339 -0.388622 -0.067642   \n",
       "249623  1.066341 -0.331764  0.454253  ...   -0.462791 -0.741062 -0.232627   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "91844   0.016463  0.119284 -0.188350  0.080262  0.028114   67.50      0  \n",
       "149421  0.104334 -0.283933 -1.175535  0.109232  0.149126   99.80      0  \n",
       "77816   0.074414  0.209207 -0.220112  0.060673  0.061285  140.00      0  \n",
       "190037  0.044649 -0.317071 -0.578125  0.296198  0.258734    1.51      0  \n",
       "249623 -1.054904 -0.122503  0.421441  0.367961  0.170138    8.07      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new2=pd.concat([df_new_0_subset, df_new_1_subset])\n",
    "df_new2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting Test and Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_O, y_resampled_O, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.5 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data With Rules 0.9649122807017544\n",
      "f1_score of Test Data With Rules 0.9666666666666666\n",
      "recall_score of Test Data With Rules 1.0\n",
      "before pruning: 117\n",
      "before pruning: 70\n",
      "Accuracy of Test Data With Rules 0.8947368421052632\n",
      "f1_score of Test Data With Rules 0.8636363636363636\n",
      "recall_score of Test Data With Rules 0.8636363636363636\n",
      "before pruning: 129\n",
      "before pruning: 70\n",
      "Accuracy of Test Data With Rules 0.8947368421052632\n",
      "f1_score of Test Data With Rules 0.8846153846153846\n",
      "recall_score of Test Data With Rules 0.7931034482758621\n",
      "before pruning: 91\n",
      "before pruning: 68\n",
      "Accuracy of Test Data With Rules 0.9473684210526315\n",
      "f1_score of Test Data With Rules 0.9491525423728813\n",
      "recall_score of Test Data With Rules 1.0\n",
      "before pruning: 73\n",
      "before pruning: 35\n",
      "Accuracy of Test Data With Rules 1.0\n",
      "f1_score of Test Data With Rules 1.0\n",
      "recall_score of Test Data With Rules 1.0\n",
      "before pruning: 211\n",
      "before pruning: 132\n",
      "Accuracy of Test Data With Rules 0.9649122807017544\n",
      "f1_score of Test Data With Rules 0.96875\n",
      "recall_score of Test Data With Rules 1.0\n",
      "before pruning: 102\n",
      "before pruning: 64\n",
      "Accuracy of Test Data With Rules 0.9298245614035088\n",
      "f1_score of Test Data With Rules 0.9393939393939394\n",
      "recall_score of Test Data With Rules 0.9393939393939394\n",
      "before pruning: 125\n",
      "before pruning: 73\n",
      "Accuracy of Test Data With Rules 0.9649122807017544\n",
      "f1_score of Test Data With Rules 0.9629629629629629\n",
      "recall_score of Test Data With Rules 0.9629629629629629\n",
      "before pruning: 117\n",
      "before pruning: 84\n",
      "Accuracy of Test Data With Rules 0.9464285714285714\n",
      "f1_score of Test Data With Rules 0.9577464788732395\n",
      "recall_score of Test Data With Rules 0.9444444444444444\n",
      "before pruning: 173\n",
      "before pruning: 111\n",
      "Accuracy of Test Data With Rules 0.9464285714285714\n",
      "f1_score of Test Data With Rules 0.9411764705882353\n",
      "recall_score of Test Data With Rules 0.9230769230769231\n",
      "before pruning: 127\n",
      "before pruning: 97\n",
      "Accuracy of Test Data With Rules 0.9649122807017544\n",
      "f1_score of Test Data With Rules 0.9565217391304348\n",
      "recall_score of Test Data With Rules 1.0\n",
      "before pruning: 113\n",
      "before pruning: 82\n",
      "Accuracy of Test Data With Rules 0.9298245614035088\n",
      "f1_score of Test Data With Rules 0.9333333333333333\n",
      "recall_score of Test Data With Rules 0.9032258064516129\n",
      "before pruning: 117\n",
      "before pruning: 76\n",
      "Accuracy of Test Data With Rules 1.0\n",
      "f1_score of Test Data With Rules 1.0\n",
      "recall_score of Test Data With Rules 1.0\n",
      "before pruning: 134\n",
      "before pruning: 84\n",
      "Accuracy of Test Data With Rules 0.9473684210526315\n",
      "f1_score of Test Data With Rules 0.9411764705882353\n",
      "recall_score of Test Data With Rules 0.9230769230769231\n",
      "before pruning: 110\n",
      "before pruning: 58\n",
      "Accuracy of Test Data With Rules 0.9473684210526315\n",
      "f1_score of Test Data With Rules 0.9473684210526316\n",
      "recall_score of Test Data With Rules 0.9310344827586207\n",
      "before pruning: 90\n",
      "before pruning: 52\n",
      "Accuracy of Test Data With Rules 0.9473684210526315\n",
      "f1_score of Test Data With Rules 0.9589041095890412\n",
      "recall_score of Test Data With Rules 0.9459459459459459\n",
      "before pruning: 102\n",
      "before pruning: 60\n",
      "Accuracy of Test Data With Rules 0.9298245614035088\n",
      "f1_score of Test Data With Rules 0.9374999999999999\n",
      "recall_score of Test Data With Rules 0.9090909090909091\n",
      "before pruning: 111\n",
      "before pruning: 73\n",
      "Accuracy of Test Data With Rules 0.9649122807017544\n",
      "f1_score of Test Data With Rules 0.9583333333333334\n",
      "recall_score of Test Data With Rules 0.9583333333333334\n",
      "before pruning: 122\n",
      "before pruning: 94\n",
      "Accuracy of Test Data With Rules 0.9642857142857143\n",
      "f1_score of Test Data With Rules 0.9615384615384615\n",
      "recall_score of Test Data With Rules 0.9259259259259259\n",
      "before pruning: 93\n",
      "before pruning: 68\n",
      "Accuracy of Test Data With Rules 0.9464285714285714\n",
      "f1_score of Test Data With Rules 0.9454545454545454\n",
      "recall_score of Test Data With Rules 0.9629629629629629\n",
      "before pruning: 144\n",
      "before pruning: 104\n",
      "Mean Accuracy So Far on Test: 0.7131735588972432\n",
      "Mean f1_score So Far on Test: 0.9487115611564843\n",
      "Mean recall_score So Far on Test: 0.9443107185668363\n",
      "The Mean Accuracy of Classifier  0.9498276942355892\n",
      "The Mean f1_score of Classifier  0.9487115611564843\n",
      "The Mean recall_score of Classifier  0.9443107185668363\n",
      "The Mean Variance of Classifier  0.0006855256385795317\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "parent_node = None\n",
    "# the node class that will make up the tree\n",
    "class decisionTreeNode():\n",
    "    def __init__(self, is_leaf_node, classification, attribute_split_value, parent, left_child, right_child, height):\n",
    "\n",
    "        self.classification = None\n",
    "        self.attribute_split = None\n",
    "        self.attribute_split_value = None\n",
    "        self.parent = parent\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.height = None\n",
    "        self.is_leaf_node = True\n",
    "\n",
    "\n",
    "\n",
    "#Split the data based on the feature and a value to data above and data below\n",
    "def split_data(data, split_column, split_value):\n",
    "    \n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    data_below = data[split_column_values <= split_value]\n",
    "    data_above = data[split_column_values >  split_value]\n",
    "    \n",
    "    return data_below, data_above\n",
    "\n",
    "#Get all the boundary values for each features (Key is feature and values are the splits)\n",
    "def get_potential_splits(data):\n",
    "    \n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    for column_index in range(n_columns - 1):        # excluding the last column which is the label\n",
    "        potential_splits[column_index] = []\n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "\n",
    "        for index in range(len(unique_values)):\n",
    "            if index != 0:\n",
    "                current_value = unique_values[index]\n",
    "                previous_value = unique_values[index - 1]\n",
    "                potential_split = (current_value + previous_value) / 2\n",
    "                \n",
    "                potential_splits[column_index].append(potential_split)\n",
    "        if (unique_values.shape[0] == 1):\n",
    "            potential_split = unique_values[index]\n",
    "            \n",
    "            potential_splits[column_index].append(potential_split)\n",
    "\n",
    "    \n",
    "    return potential_splits\n",
    "\n",
    "#Calculates Entropy of the data given\n",
    "def calculate_entropy(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "     \n",
    "    return entropy\n",
    "\n",
    "#Calculates the entropy of data below and data above\n",
    "def calculate_overall_entropy(data_below, data_above):\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_entropy =  (p_data_below * calculate_entropy(data_below) \n",
    "                      + p_data_above * calculate_entropy(data_above))\n",
    "    \n",
    "    return overall_entropy\n",
    "\n",
    "#Check if all data is of same class\n",
    "def check_purity(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes = np.unique(label_column)\n",
    "\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#Classify data based on majority\n",
    "def classify_data(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    index = counts_unique_classes.argmax()\n",
    "    classification = unique_classes[index]\n",
    "    \n",
    "    return classification\n",
    "\n",
    "#Gives the best feature and its split value after checking all features based on gain ratio\n",
    "def determine_best_split(data, potential_splits):\n",
    "    \n",
    "    entropy_label = calculate_entropy(data)   \n",
    "    overall_gain = -1.0\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n",
    "            current_information_gain = entropy_label - current_overall_entropy\n",
    "            current_splitting_info = splitting_information(data_below,data_above)\n",
    "            if current_splitting_info == 0:\n",
    "                current_gain_ratio = 0\n",
    "            else:\n",
    "                current_gain_ratio = float(current_information_gain / current_splitting_info)\n",
    "\n",
    "            if current_gain_ratio >= overall_gain:\n",
    "                overall_gain = current_gain_ratio\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value\n",
    "\n",
    "#Calculates the splitting Info of data above and below for that boundary value\n",
    "def splitting_information(data_below,data_above):\n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below)/ n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    if p_data_below == 0:\n",
    "        splitting_info = p_data_above * np.log2(p_data_above)\n",
    "    elif p_data_above == 0:\n",
    "        splitting_info = p_data_below * np.log2(p_data_below)\n",
    "    else:\n",
    "        splitting_info = -p_data_below * np.log2(p_data_below) -p_data_above * np.log2(p_data_above) \n",
    "    \n",
    "    return splitting_info\n",
    "\n",
    "def decision_tree_algorithm(df, parent_node,counter=0, min_samples=3):\n",
    "    node = decisionTreeNode(True, None, None, parent_node, None, None, 0)\n",
    "    # data preparations\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        data = df.values\n",
    "    else:\n",
    "        data = df           \n",
    "    \n",
    "    \n",
    "    # base cases\n",
    "    if (check_purity(data)) or (len(data) < min_samples):\n",
    "        classification = classify_data(data)\n",
    "        node.is_leaf_node = True\n",
    "        node.classification = classification\n",
    "        return node\n",
    "\n",
    "    \n",
    "    # recursive part\n",
    "    else:    \n",
    "        counter += 1\n",
    "\n",
    "        # helper functions \n",
    "        potential_splits = get_potential_splits(data)\n",
    "        split_column, split_value = determine_best_split(data, potential_splits)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        node.is_leaf_node = False\n",
    "        # instantiate sub-tree\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        \n",
    "        question = \"{} <= {}\".format(feature_name, split_value)\n",
    "\n",
    "\n",
    "       \n",
    "        if (parent_node == None):\n",
    "            node.height = 0\n",
    "        else:\n",
    "            node.parent = parent_node\n",
    "            node.height = node.parent.height + 1\n",
    "\n",
    "\n",
    "        node.attribute_split = feature_name\n",
    "        node.attribute_split_value = split_value\n",
    "\n",
    "        # find answers (recursion)\n",
    "        node.left_child = decision_tree_algorithm(data_below,node, counter, min_samples)\n",
    "        node.right_child = decision_tree_algorithm(data_above,node, counter, min_samples)\n",
    "        \n",
    "\n",
    "        return node\n",
    "\n",
    "def get_paths(root, path, pathlen,all_paths,val):\n",
    "    if (root==None):\n",
    "        return\n",
    "    \n",
    "    if root.is_leaf_node == True: \n",
    "        path.append(root.classification) \n",
    "    else:\n",
    "        path.append('row[\\'' + root.attribute_split + '\\']' + val + str(root.attribute_split_value))\n",
    "        \n",
    "    pathlen= pathlen+1\n",
    "    if (root.left_child == None and root.right_child == None): # If leaf, append current path\n",
    "        add = path[:]\n",
    "        all_paths.append(add)\n",
    "        path.pop()\n",
    "        root = root.parent\n",
    "    else:\n",
    "        get_paths(root.left_child, path, pathlen,all_paths,' <= ')\n",
    "        path[pathlen-1]= 'row[\\'' + root.attribute_split + '\\']' +' > ' + str(root.attribute_split_value)\n",
    "        get_paths(root.right_child, path,pathlen,all_paths,' <= ')\n",
    "        path.pop()\n",
    "\n",
    "    return all_paths\n",
    "\n",
    "def classify_test_data(root,data):\n",
    "    predictions = []\n",
    "    tree = root \n",
    "    data = data.iloc[:, :-1]\n",
    "    for index, sample in data.iterrows():\n",
    "        root = tree\n",
    "        while(tree.is_leaf_node!=True):\n",
    "            if (sample.loc[tree.attribute_split] <= tree.attribute_split_value):\n",
    "                tree = tree.left_child\n",
    "            else:\n",
    "                tree = tree.right_child\n",
    "        predictions.append(tree.classification)\n",
    "        tree = root\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def calc_accuracy_rule(rule,test):\n",
    "    wrong = 0\n",
    "#Check how many classified correctly\n",
    "    for index, row in test.iterrows():\n",
    "        s=0\n",
    "        while(s<len(rule)-1):\n",
    "            if (eval(rule[s])== False):\n",
    "                wrong += 1\n",
    "                break\n",
    "            s=s+1\n",
    "    #Initial Accuracy of one Rule before pruning\n",
    "    accuracy = (test.shape[0]-wrong) / test.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def recursive_len(item):\n",
    "    if type(item) == list:\n",
    "        return sum(recursive_len(subitem) for subitem in item)\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def prune(all_rules,val_data):\n",
    "    acc_rlist = []\n",
    "    maping = []\n",
    "    rulenos = []\n",
    "    #What are the labels in my val data\n",
    "    ctoprune = val_data['Class'].unique()\n",
    "#     Loop at all rules one by one\n",
    "    size_of_rules = recursive_len(all_rules)\n",
    "    print(\"before pruning:\",size_of_rules)\n",
    "    for i in range(len(all_rules)):\n",
    "        init_accuracy = 0\n",
    "        #Loop only on the rules applicable to my valset\n",
    "        if all_rules[i][-1] in ctoprune:\n",
    "                #Get the label of the Rule\n",
    "                label = all_rules[i][-1]\n",
    "                #Get all samples for that label\n",
    "                test = val_data[val_data['Class']==label]\n",
    "                #Check Initial Accuracy of the rule\n",
    "                init_accuracy = calc_accuracy_rule(all_rules[i],test)\n",
    "                \n",
    "                temp = all_rules[i][:]\n",
    "                pruned_accuracy = -1\n",
    "                while (init_accuracy!=pruned_accuracy):\n",
    "                # if (init_accuracy!=pruned_accuracy):\n",
    "                    for x in range(len(all_rules[i])-1):\n",
    "                        del temp[x]\n",
    "                        accuracy = calc_accuracy_rule(temp,test)\n",
    "                        if accuracy > init_accuracy:\n",
    "                            delx = x\n",
    "                            init_accuracy = accuracy\n",
    "                        temp = all_rules[i][:]\n",
    "                    # Ensure variable is defined\n",
    "                    try:\n",
    "                        delx\n",
    "                    except NameError:\n",
    "                        delx = None\n",
    "\n",
    "                    if delx is not None:\n",
    "                        del all_rules[i][delx]\n",
    "                        del delx\n",
    "                        # pruned_accuracy = init_accuracy\n",
    "                        if (len(all_rules[i])== 2):\n",
    "                            pruned_accuracy = init_accuracy\n",
    "                    else:\n",
    "                        pruned_accuracy = init_accuracy\n",
    "        else:\n",
    "            pruned_accuracy = init_accuracy\n",
    "        acc_rlist.append(pruned_accuracy)\n",
    "        rulenos.append(i)\n",
    "    maping.append(acc_rlist)\n",
    "    maping.append(rulenos)\n",
    "    size_of_rules = recursive_len(all_rules)\n",
    "    print(\"before pruning:\",size_of_rules)\n",
    "    maping= np.array(maping)\n",
    "    maping = pd.DataFrame(maping.T)\n",
    "    maping = maping.sort_values(0,ascending=False)\n",
    "    maping = pd.DataFrame(maping)\n",
    "    return all_rules,maping\n",
    "                \n",
    "def predict_prunedtree(all_rules,test_data,maping):\n",
    "    answer = []\n",
    "    unclassified = 0\n",
    "    for index, row in test_data.iterrows():\n",
    "        for indo,valus in maping.iterrows():\n",
    "            rule = all_rules[int(valus[1])]\n",
    "            s=0\n",
    "            count = 0\n",
    "            while(s<len(rule)-1):\n",
    "                if (eval(rule[s])== True):\n",
    "                    count = count + 1\n",
    "                s = s+1\n",
    "            if (count == len(rule)-1):\n",
    "                prediction = rule[-1]\n",
    "                break\n",
    "        try:\n",
    "            prediction\n",
    "        except NameError:\n",
    "            prediction = None\n",
    "        if prediction is not None:\n",
    "            answer.append(prediction)\n",
    "            del prediction\n",
    "        else:\n",
    "            unclassified = unclassified + 1\n",
    "#     print('Unclassified Sample',unclassified)\n",
    "    return answer\n",
    "\n",
    "def predict_preprunedtree(all_rules,test_data):\n",
    "    answer = []\n",
    "    unclassified = 0\n",
    "    for index, row in test_data.iterrows():\n",
    "        # prediction = row[-1]\n",
    "        for rule in all_rules:\n",
    "            s=0\n",
    "            count = 0\n",
    "            while(s<len(rule)-1):\n",
    "                if (eval(rule[s])== True):\n",
    "                    count = count + 1\n",
    "                s = s+1\n",
    "            if (count == len(rule)-1):\n",
    "                prediction = rule[-1]\n",
    "                break\n",
    "        try:\n",
    "            prediction\n",
    "        except NameError:\n",
    "            prediction = None\n",
    "        if prediction is not None:\n",
    "            answer.append(prediction)\n",
    "            del prediction\n",
    "        else:\n",
    "            unclassified = unclassified + 1\n",
    "#     print('Unclassified Sample',unclassified)\n",
    "    return answer\n",
    "\n",
    "def add_noise2(num,data):\n",
    "\n",
    "    siz_d = data.shape[0]\n",
    "    indx = int((num * siz_d)/100)\n",
    "    for x in range(indx):\n",
    "        pick = random.randint(0,int(siz_d/2))\n",
    "        label = data.iloc[pick:,-1].values[0]\n",
    "        if label > 0:\n",
    "            data.iloc[pick:,-1] = data.iloc[pick:,-1] - 1\n",
    "        else:\n",
    "            data.iloc[pick:,-1] = data.iloc[pick:,-1] + 1\n",
    "    return data\n",
    "\n",
    "def add_noise1(num,data):\n",
    "    siz_d = data.shape[0]\n",
    "    indx = int((num * siz_d)/100)\n",
    "    count = 0\n",
    "    for x in range(indx):\n",
    "        count = count+1\n",
    "        pick = random.randint(0,siz_d)\n",
    "        label = data.iloc[pick:,-1].values[0]\n",
    "        if label > 0:\n",
    "            label = label + 1\n",
    "        else:\n",
    "            label = label - 1\n",
    "        last = data.shape[0]  \n",
    "        data = data.append(data.iloc[pick,:])\n",
    "        data.iloc[last,-1] = label\n",
    "    return data\n",
    "\n",
    "noise1_5 = []\n",
    "noise1_10 = []\n",
    "noise1_15 = []\n",
    "noise2_5 = []\n",
    "noise2_10 = []\n",
    "noise2_15 = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    meanacc= []\n",
    "    meanf1_score= []\n",
    "    meanrecall_score= []\n",
    "    maping = []\n",
    "    mean_bprun = []\n",
    "    mean_bprun1 = []\n",
    "    mean_bprun2 = []\n",
    "    rkf = RepeatedKFold(n_splits=10, n_repeats=2, random_state=2652124)\n",
    "    for train_index, test_index in rkf.split(df_new2):\n",
    "        parent_node = None\n",
    "        test_data = df_new2.iloc[test_index]\n",
    "        train_data,val_data= train_test_split(df_new2.iloc[train_index], test_size=0.2)\n",
    "        \n",
    "        tree = decision_tree_algorithm(train_data,parent_node)\n",
    "        all_rules = get_paths(tree,[],0,[],' <= ')\n",
    "\n",
    "\n",
    "        \n",
    "# #         #Validation\n",
    "# #         y_true= val_data.iloc[:,-1].values\n",
    "# #         y_pred = classify_test_data(tree,val_data)\n",
    "# #         accuracy = accuracy_score(y_true, y_pred)\n",
    "# #         print('Pre-Pruning Accuracy of Val data With Just Tree',accuracy)\n",
    "        \n",
    "#         # # Pre-Pruning Accuracy of Test Data With Tree\n",
    "#         # y_true= test_data.iloc[:,-1].values\n",
    "#         # y_pred = classify_test_data(tree,test_data)\n",
    "#         # accuracy = accuracy_score(y_true, y_pred)\n",
    "#         # print('Pre-Pruning Accuracy of Test data With Just Tree',accuracy)\n",
    "        accuracy = []\n",
    "    \n",
    "        #Pre-Pruning Accuracy of Test Data With Rules\n",
    "        y_true= test_data.iloc[:,-1].values\n",
    "        answer = predict_preprunedtree(all_rules,test_data)\n",
    "        accuracy = accuracy_score(y_true, answer)\n",
    "        f1_sc = f1_score(y_true, answer)\n",
    "        recall_sc = recall_score(y_true, answer)\n",
    "        print('Accuracy of Test Data With Rules',accuracy)\n",
    "        print('f1_score of Test Data With Rules',f1_sc)\n",
    "        print('recall_score of Test Data With Rules',recall_sc)\n",
    "        #print('Number of Rules before pruning',len(all_rules))\n",
    "        mean_bprun.append(accuracy)\n",
    "        mean_bprun1.append(f1_sc)\n",
    "        mean_bprun2.append(recall_sc)\n",
    "        #Post Pruning Accuracy with Rules\n",
    "        all_rules,maping = prune(all_rules,val_data)\n",
    "        #print('Number of Rules after pruning',len(all_rules))\n",
    "        answer = predict_prunedtree(all_rules,test_data,maping)\n",
    "        # y_true= test_data.iloc[:,-1].values\n",
    "        accuracy = accuracy_score(y_true, answer)\n",
    "        #print('Post-Pruned Accuracy Without Noise',accuracy)\n",
    "        meanacc.append(accuracy)\n",
    "        meanf1_score.append(f1_sc)\n",
    "        meanrecall_score.append(recall_sc)\n",
    "\n",
    "\n",
    "  \n",
    "   \n",
    "    print('Mean Accuracy So Far on Test:',sum(meanacc) / len(meanacc))\n",
    "    print('Mean f1_score So Far on Test:',sum(meanf1_score) / len(meanf1_score))\n",
    "    print('Mean recall_score So Far on Test:',sum(meanrecall_score) / len(meanrecall_score))\n",
    "    print('The Mean Accuracy of Classifier ',sum(mean_bprun) / len(mean_bprun))\n",
    "    print('The Mean f1_score of Classifier ',sum(mean_bprun1) / len(mean_bprun1))\n",
    "    print('The Mean recall_score of Classifier ',sum(mean_bprun2) / len(mean_bprun2))\n",
    "    print('The Mean Variance of Classifier ',np.var(np.array(mean_bprun)))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243249</th>\n",
       "      <td>151854.0</td>\n",
       "      <td>0.695087</td>\n",
       "      <td>-2.570941</td>\n",
       "      <td>-1.623645</td>\n",
       "      <td>0.686052</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.129365</td>\n",
       "      <td>0.515870</td>\n",
       "      <td>-0.175081</td>\n",
       "      <td>0.678933</td>\n",
       "      <td>...</td>\n",
       "      <td>1.184244</td>\n",
       "      <td>0.411233</td>\n",
       "      <td>-0.272745</td>\n",
       "      <td>-0.436602</td>\n",
       "      <td>-0.434089</td>\n",
       "      <td>-0.527159</td>\n",
       "      <td>0.477314</td>\n",
       "      <td>-0.193508</td>\n",
       "      <td>0.047603</td>\n",
       "      <td>693.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53653</th>\n",
       "      <td>46088.0</td>\n",
       "      <td>1.344553</td>\n",
       "      <td>-0.569670</td>\n",
       "      <td>0.069459</td>\n",
       "      <td>-0.814642</td>\n",
       "      <td>-0.757287</td>\n",
       "      <td>-0.660990</td>\n",
       "      <td>-0.438410</td>\n",
       "      <td>-0.125444</td>\n",
       "      <td>-1.123910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148648</td>\n",
       "      <td>0.260735</td>\n",
       "      <td>0.579145</td>\n",
       "      <td>-0.194078</td>\n",
       "      <td>0.037350</td>\n",
       "      <td>0.629855</td>\n",
       "      <td>-0.116573</td>\n",
       "      <td>-0.016747</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>43.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140721</th>\n",
       "      <td>83890.0</td>\n",
       "      <td>1.069853</td>\n",
       "      <td>0.071475</td>\n",
       "      <td>0.484912</td>\n",
       "      <td>1.346547</td>\n",
       "      <td>-0.249256</td>\n",
       "      <td>0.017255</td>\n",
       "      <td>-0.031922</td>\n",
       "      <td>0.135633</td>\n",
       "      <td>0.077774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173677</td>\n",
       "      <td>-0.029689</td>\n",
       "      <td>0.050721</td>\n",
       "      <td>-0.061816</td>\n",
       "      <td>0.221879</td>\n",
       "      <td>0.584474</td>\n",
       "      <td>-0.342960</td>\n",
       "      <td>0.030084</td>\n",
       "      <td>0.010263</td>\n",
       "      <td>27.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152136</th>\n",
       "      <td>96953.0</td>\n",
       "      <td>1.845394</td>\n",
       "      <td>0.311220</td>\n",
       "      <td>0.276847</td>\n",
       "      <td>4.045099</td>\n",
       "      <td>-0.221962</td>\n",
       "      <td>0.279406</td>\n",
       "      <td>-0.571115</td>\n",
       "      <td>0.022169</td>\n",
       "      <td>1.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299907</td>\n",
       "      <td>0.107481</td>\n",
       "      <td>0.591033</td>\n",
       "      <td>0.119729</td>\n",
       "      <td>-0.194731</td>\n",
       "      <td>-0.196962</td>\n",
       "      <td>0.069587</td>\n",
       "      <td>-0.022024</td>\n",
       "      <td>-0.037917</td>\n",
       "      <td>26.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273033</th>\n",
       "      <td>165388.0</td>\n",
       "      <td>2.048592</td>\n",
       "      <td>-0.110847</td>\n",
       "      <td>-1.206953</td>\n",
       "      <td>0.208453</td>\n",
       "      <td>0.101991</td>\n",
       "      <td>-0.667002</td>\n",
       "      <td>0.069879</td>\n",
       "      <td>-0.160737</td>\n",
       "      <td>0.313878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204730</td>\n",
       "      <td>-0.264155</td>\n",
       "      <td>-0.660785</td>\n",
       "      <td>0.280766</td>\n",
       "      <td>-0.407540</td>\n",
       "      <td>-0.277268</td>\n",
       "      <td>0.202873</td>\n",
       "      <td>-0.075116</td>\n",
       "      <td>-0.074302</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "243249  151854.0  0.695087 -2.570941 -1.623645  0.686052 -0.967187 -0.129365   \n",
       "53653    46088.0  1.344553 -0.569670  0.069459 -0.814642 -0.757287 -0.660990   \n",
       "140721   83890.0  1.069853  0.071475  0.484912  1.346547 -0.249256  0.017255   \n",
       "152136   96953.0  1.845394  0.311220  0.276847  4.045099 -0.221962  0.279406   \n",
       "273033  165388.0  2.048592 -0.110847 -1.206953  0.208453  0.101991 -0.667002   \n",
       "\n",
       "              V7        V8        V9   ...         V20       V21       V22  \\\n",
       "243249  0.515870 -0.175081  0.678933   ...    1.184244  0.411233 -0.272745   \n",
       "53653  -0.438410 -0.125444 -1.123910   ...    0.148648  0.260735  0.579145   \n",
       "140721 -0.031922  0.135633  0.077774   ...   -0.173677 -0.029689  0.050721   \n",
       "152136 -0.571115  0.022169  1.002635   ...   -0.299907  0.107481  0.591033   \n",
       "273033  0.069879 -0.160737  0.313878   ...   -0.204730 -0.264155 -0.660785   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \n",
       "243249 -0.436602 -0.434089 -0.527159  0.477314 -0.193508  0.047603  693.20  \n",
       "53653  -0.194078  0.037350  0.629855 -0.116573 -0.016747  0.002344   43.10  \n",
       "140721 -0.061816  0.221879  0.584474 -0.342960  0.030084  0.010263   27.62  \n",
       "152136  0.119729 -0.194731 -0.196962  0.069587 -0.022024 -0.037917   26.97  \n",
       "273033  0.280766 -0.407540 -0.277268  0.202873 -0.075116 -0.074302    0.89  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new2 = df_new2.iloc[:,:-1]\n",
    "y_new2 = df_new2.iloc[:,-1]\n",
    "X_new2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new2, y_new2, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:  15 train:  1.0\n",
      "depth:  15 test:  0.973404255319149\n"
     ]
    }
   ],
   "source": [
    "max_test_score = 0\n",
    "MaxDepth=[]\n",
    "TestScore=[]\n",
    "TrainScore=[]\n",
    "for i in range(1, 20):\n",
    "    MaxDepth.append(i)\n",
    "    clf_dt = DecisionTreeClassifier(max_depth=i)\n",
    "    clf_dt.fit(X_train, y_train)\n",
    "    train_score = clf_dt.score(X_train, y_train)\n",
    "    test_score = clf_dt.score(X_test, y_test)\n",
    "    TestScore.append(test_score)\n",
    "    TrainScore.append(train_score)\n",
    "    if test_score > max_test_score:\n",
    "        related_train_score = train_score\n",
    "        max_test_score = test_score\n",
    "        max_i = i\n",
    "        best_clf_dt = clf_dt\n",
    "print(\"depth: \", max_i, \"train: \", related_train_score)\n",
    "print(\"depth: \", max_i, \"test: \", max_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9763157894736842"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gini = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=15, max_features=None, max_leaf_nodes=None, min_samples_leaf=5, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best')\n",
    "clf_gini.fit(X_train, y_train)\n",
    "clf_gini.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_en_gini = clf_gini.predict(X_test)\n",
    "y_pred_en_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[97  3]\n",
      " [ 8 80]]\n",
      "\t\tConfusion matrix for CART\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95       100\n",
      "           1       0.96      0.91      0.94        88\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       188\n",
      "   macro avg       0.94      0.94      0.94       188\n",
      "weighted avg       0.94      0.94      0.94       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_en_gini))  \n",
    "print(\"\\t\\tConfusion matrix for CART\")\n",
    "print(classification_report(y_test, y_pred_en_gini)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
